{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use model Unet\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 96, 96]             320\n",
      "       BatchNorm2d-2           [-1, 32, 96, 96]              64\n",
      "              ReLU-3           [-1, 32, 96, 96]               0\n",
      "         Dropout2d-4           [-1, 32, 96, 96]               0\n",
      "            Conv2d-5           [-1, 32, 96, 96]           9,248\n",
      "       BatchNorm2d-6           [-1, 32, 96, 96]              64\n",
      "              ReLU-7           [-1, 32, 96, 96]               0\n",
      "         Dropout2d-8           [-1, 32, 96, 96]               0\n",
      "       double_conv-9           [-1, 32, 96, 96]               0\n",
      "        MaxPool2d-10           [-1, 32, 48, 48]               0\n",
      "           Conv2d-11           [-1, 64, 48, 48]          18,496\n",
      "      BatchNorm2d-12           [-1, 64, 48, 48]             128\n",
      "             ReLU-13           [-1, 64, 48, 48]               0\n",
      "        Dropout2d-14           [-1, 64, 48, 48]               0\n",
      "           Conv2d-15           [-1, 64, 48, 48]          36,928\n",
      "      BatchNorm2d-16           [-1, 64, 48, 48]             128\n",
      "             ReLU-17           [-1, 64, 48, 48]               0\n",
      "        Dropout2d-18           [-1, 64, 48, 48]               0\n",
      "      double_conv-19           [-1, 64, 48, 48]               0\n",
      "        MaxPool2d-20           [-1, 64, 24, 24]               0\n",
      "           Conv2d-21          [-1, 128, 24, 24]          73,856\n",
      "      BatchNorm2d-22          [-1, 128, 24, 24]             256\n",
      "             ReLU-23          [-1, 128, 24, 24]               0\n",
      "        Dropout2d-24          [-1, 128, 24, 24]               0\n",
      "           Conv2d-25          [-1, 128, 24, 24]         147,584\n",
      "      BatchNorm2d-26          [-1, 128, 24, 24]             256\n",
      "             ReLU-27          [-1, 128, 24, 24]               0\n",
      "        Dropout2d-28          [-1, 128, 24, 24]               0\n",
      "      double_conv-29          [-1, 128, 24, 24]               0\n",
      "        MaxPool2d-30          [-1, 128, 12, 12]               0\n",
      "           Conv2d-31          [-1, 256, 14, 14]         295,168\n",
      "      BatchNorm2d-32          [-1, 256, 14, 14]             512\n",
      "             ReLU-33          [-1, 256, 14, 14]               0\n",
      "        Dropout2d-34          [-1, 256, 14, 14]               0\n",
      "           Conv2d-35          [-1, 256, 16, 16]         590,080\n",
      "      BatchNorm2d-36          [-1, 256, 16, 16]             512\n",
      "             ReLU-37          [-1, 256, 16, 16]               0\n",
      "        Dropout2d-38          [-1, 256, 16, 16]               0\n",
      "      double_conv-39          [-1, 256, 16, 16]               0\n",
      "  ConvTranspose2d-40          [-1, 128, 32, 32]         131,200\n",
      "           Conv2d-41          [-1, 128, 26, 26]         295,040\n",
      "      BatchNorm2d-42          [-1, 128, 26, 26]             256\n",
      "             ReLU-43          [-1, 128, 26, 26]               0\n",
      "        Dropout2d-44          [-1, 128, 26, 26]               0\n",
      "           Conv2d-45          [-1, 128, 28, 28]         147,584\n",
      "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
      "             ReLU-47          [-1, 128, 28, 28]               0\n",
      "        Dropout2d-48          [-1, 128, 28, 28]               0\n",
      "      double_conv-49          [-1, 128, 28, 28]               0\n",
      "  ConvTranspose2d-50           [-1, 64, 56, 56]          32,832\n",
      "           Conv2d-51           [-1, 64, 50, 50]          73,792\n",
      "      BatchNorm2d-52           [-1, 64, 50, 50]             128\n",
      "             ReLU-53           [-1, 64, 50, 50]               0\n",
      "        Dropout2d-54           [-1, 64, 50, 50]               0\n",
      "           Conv2d-55           [-1, 64, 52, 52]          36,928\n",
      "      BatchNorm2d-56           [-1, 64, 52, 52]             128\n",
      "             ReLU-57           [-1, 64, 52, 52]               0\n",
      "        Dropout2d-58           [-1, 64, 52, 52]               0\n",
      "      double_conv-59           [-1, 64, 52, 52]               0\n",
      "  ConvTranspose2d-60         [-1, 32, 104, 104]           8,224\n",
      "           Conv2d-61           [-1, 32, 98, 98]          18,464\n",
      "      BatchNorm2d-62           [-1, 32, 98, 98]              64\n",
      "             ReLU-63           [-1, 32, 98, 98]               0\n",
      "        Dropout2d-64           [-1, 32, 98, 98]               0\n",
      "           Conv2d-65         [-1, 32, 100, 100]           9,248\n",
      "      BatchNorm2d-66         [-1, 32, 100, 100]              64\n",
      "             ReLU-67         [-1, 32, 100, 100]               0\n",
      "        Dropout2d-68         [-1, 32, 100, 100]               0\n",
      "      double_conv-69         [-1, 32, 100, 100]               0\n",
      "           Conv2d-70          [-1, 1, 100, 100]              33\n",
      "================================================================\n",
      "Total params: 1,927,841\n",
      "Trainable params: 1,927,841\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 85.24\n",
      "Params size (MB): 7.35\n",
      "Estimated Total Size (MB): 92.63\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "\n",
    "# in order to get reproducable results\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "import cmapy\n",
    "from pytorchtools import EarlyStopping\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # Only GPU 3 is visible to this code\n",
    "time1 = time.time()\n",
    "\n",
    "data_folder = \"/data/anyu/thebeData\"\n",
    "data_path = \"{}/processedThebe\".format(data_folder)\n",
    "\n",
    "best_model_fpath = 'unet_96_48_seed_test.model'\n",
    "best_iou_threshold=0.5\n",
    "epoches = 100\n",
    "patience = 20\n",
    "\n",
    "modelNo = \"unet\"\n",
    "if modelNo == \"unet\":\n",
    "    from model_zoo.UNET import Unet\n",
    "    model = Unet()\n",
    "    print(\"use model Unet\")\n",
    "elif modelNo == \"deeplab\":\n",
    "    from model_zoo.DEEPLAB.deeplab import DeepLab\n",
    "    model = DeepLab(backbone='mobilenet', num_classes=1, output_stride=16)\n",
    "    print(\"use model DeepLab\")\n",
    "elif modelNo == \"hed\":\n",
    "    from model_zoo.HED import HED\n",
    "    model = HED()\n",
    "    print(\"use model HED\")\n",
    "elif modelNo == \"rcf\":\n",
    "    from model_zoo.RCF import RCF\n",
    "    model = RCF()\n",
    "    print(\"use model RCF\")\n",
    "else:\n",
    "    print(\"please select a valid model\")\n",
    "model.cuda();\n",
    "summary(model, (1, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "from os.path import splitext\n",
    "from os import listdir\n",
    "from glob import glob\n",
    "class faultsDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, imgs_dir, masks_dir, scale=1, mask_suffix=''):\n",
    "\n",
    "    def __init__(self, imgs_dir, masks_dir):\n",
    "#         self.train = train\n",
    "        self.images_dir = imgs_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.ids = [splitext(file)[0] for file in listdir(imgs_dir) if not file.startswith('.')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        idx = self.ids[i]\n",
    "        mask = np.load(\"{}/{}.npy\".format(self.masks_dir,idx))\n",
    "        img = np.load(\"{}/{}.npy\".format(self.images_dir,idx))\n",
    "#         mask_file = glob(self.masks_dir + idx + '.npy')\n",
    "#         img_file = glob(self.images_dir + idx + '.npy')\n",
    "\n",
    "#         assert len(mask_file) == 1, \\\n",
    "#             f'Either no mask or multiple masks found for the ID {idx}: {mask_file}'\n",
    "#         assert len(img_file) == 1, \\\n",
    "#             f'Either no image or multiple images found for the ID {idx}: {img_file}'\n",
    "#         mask = np.load(mask_file[0])\n",
    "#         img = np.load(img_file[0])\n",
    "\n",
    "        assert img.size == mask.size, \\\n",
    "            f'Image and mask {idx} should be the same size, but are {img.size} and {mask.size}'\n",
    "\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "\n",
    "        return (img, mask)\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "faults_dataset_train = faultsDataset(imgs_dir = \"{}/train/seismic\".format(data_path), masks_dir= \"{}/train/annotation\".format(data_path))\n",
    "faults_dataset_val = faultsDataset(imgs_dir = \"{}/val/seismic\".format(data_path), masks_dir= \"{}/val/annotation\".format(data_path))\n",
    "\n",
    "batch_size = 64 \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=faults_dataset_train, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=faults_dataset_val, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6, momentum=0.9, weight_decay=0.0002)\n",
    "if modelNo == \"hed\" or modelNo == \"rcf\":\n",
    "    print(\"optimizer = torch.optim.SGD(model.parameters(), lr=1e-6, momentum=0.9, weight_decay=0.0002)\")\n",
    "if modelNo == \"unet\" or modelNo == \"deeplab\":\n",
    "    print(\"optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.1, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.235574).  Saving model ...\n",
      "Epoch: 1. Train Loss: 0.24820204079151154. Val Loss: 0.23557445406913757. Train IoU: 0.20784807205200195. Val IoU: 0.24321377277374268. Time: 892.1702287197113. LR: 0.01\n"
     ]
    }
   ],
   "source": [
    "bceloss = nn.BCELoss()\n",
    "mean_train_losses = []\n",
    "mean_val_losses = []\n",
    "mean_train_accuracies = []\n",
    "mean_val_accuracies = []\n",
    "t_start = time.time()\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True, delta = 0)\n",
    "for epoch in range(epoches):                  \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    labelled_val_accuracies = []\n",
    "\n",
    "    model.train()\n",
    "    for images, masks in train_loader: \n",
    "        torch.cuda.empty_cache()\n",
    "        images = Variable(images.cuda())\n",
    "        masks = Variable(masks.cuda())\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = torch.zeros(1).cuda()\n",
    "        y_preds = outputs\n",
    "        if modelNo == \"unet\" or modelNo == \"deeplab\":\n",
    "            loss = bceloss(outputs, masks) \n",
    "        elif modelNo == \"hed\":\n",
    "            for o in range(5):\n",
    "                loss = loss + cross_entropy_loss_HED(outputs[o], masks)\n",
    "            loss = loss + bceloss(outputs[-1],masks)\n",
    "            y_preds = outputs[-1]\n",
    "        elif modelNo == \"rcf\":\n",
    "            for o in outputs:\n",
    "                loss = loss + cross_entropy_loss_RCF(o, masks)\n",
    "            y_preds = outputs[-1]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_losses.append(loss.data)\n",
    "        predicted_mask = y_preds > best_iou_threshold\n",
    "        train_acc = iou_pytorch(predicted_mask.squeeze(1).byte(), masks.squeeze(1).byte())\n",
    "        train_accuracies.append(train_acc.mean())        \n",
    "\n",
    "    model.eval()\n",
    "    for images, masks in val_loader:\n",
    "        torch.cuda.empty_cache()\n",
    "        images = Variable(images.cuda())\n",
    "        masks = Variable(masks.cuda())\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = torch.zeros(1).cuda()\n",
    "        y_preds = outputs\n",
    "        if modelNo == \"unet\" or modelNo == \"deeplab\":\n",
    "            loss = bceloss(outputs, masks) \n",
    "        elif modelNo == \"hed\":\n",
    "            for o in range(5):\n",
    "                loss = loss + cross_entropy_loss_HED(outputs[o], masks)\n",
    "            loss = loss + bceloss(outputs[-1],masks)\n",
    "            y_preds = outputs[-1]\n",
    "        elif modelNo == \"rcf\":\n",
    "            for o in outputs:\n",
    "                loss = loss + cross_entropy_loss_RCF(o, masks)\n",
    "            y_preds = outputs[-1]\n",
    "        val_losses.append(loss.data)\n",
    "        predicted_mask = y_preds > best_iou_threshold\n",
    "        val_acc = iou_pytorch(predicted_mask.byte(), masks.squeeze(1).byte())\n",
    "        val_accuracies.append(val_acc.mean())\n",
    "\n",
    "        \n",
    "    mean_train_losses.append(torch.mean(torch.stack(train_losses)))\n",
    "    mean_val_losses.append(torch.mean(torch.stack(val_losses)))\n",
    "    mean_train_accuracies.append(torch.mean(torch.stack(train_accuracies)))\n",
    "    mean_val_accuracies.append(torch.mean(torch.stack(val_accuracies)))\n",
    "    \n",
    "    scheduler.step(torch.mean(torch.stack(val_losses)))    \n",
    "    early_stopping(torch.mean(torch.stack(val_losses)), model, best_model_fpath)\n",
    "    \n",
    "\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "        \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        learningRate = param_group['lr']\n",
    "    \n",
    "    \n",
    "    # Print Epoch results\n",
    "    t_end = time.time()\n",
    "\n",
    "    print('Epoch: {}. Train Loss: {}. Val Loss: {}. Train IoU: {}. Val IoU: {}. Time: {}. LR: {}'\n",
    "          .format(epoch+1, torch.mean(torch.stack(train_losses)), torch.mean(torch.stack(val_losses)), torch.mean(torch.stack(train_accuracies)), torch.mean(torch.stack(val_accuracies)), t_end-t_start, learningRate))\n",
    "    \n",
    "    t_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_losses = np.asarray(torch.stack(mean_train_losses).cpu())\n",
    "mean_val_losses = np.asarray(torch.stack(mean_val_losses).cpu())\n",
    "mean_train_accuracies = np.asarray(torch.stack(mean_train_accuracies).cpu())\n",
    "mean_val_accuracies = np.asarray(torch.stack(mean_val_accuracies).cpu())\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "train_loss_series = pd.Series(mean_train_losses)\n",
    "val_loss_series = pd.Series(mean_val_losses)\n",
    "train_loss_series.plot(label=\"train_loss\")\n",
    "val_loss_series.plot(label=\"validation_loss\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "train_acc_series = pd.Series(mean_train_accuracies)\n",
    "val_acc_series = pd.Series(mean_val_accuracies)\n",
    "train_acc_series.plot(label=\"train_acc\")\n",
    "val_acc_series.plot(label=\"validation_acc\")\n",
    "plt.legend()\n",
    "plt.savefig('{}_loss_acc.png'.format(best_model_fpath))\n",
    "\n",
    "totaltime = time.time()-time1\n",
    "print(\"total cost {} hours\".format(totaltime/3600))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
