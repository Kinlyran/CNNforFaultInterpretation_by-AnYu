nohup: ignoring input
]0;IPython: an/all_model_newload in 28.464758157730103 sec
(1100, 1537, 3174) (1100, 1537, 3174)
1.0 0.0 True False
276
use model DeepLab
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 48, 48]             288
            Conv2d-2           [-1, 32, 48, 48]             288
SynchronizedBatchNorm2d-3           [-1, 32, 48, 48]              64
SynchronizedBatchNorm2d-4           [-1, 32, 48, 48]              64
             ReLU6-5           [-1, 32, 48, 48]               0
             ReLU6-6           [-1, 32, 48, 48]               0
            Conv2d-7           [-1, 32, 48, 48]             288
            Conv2d-8           [-1, 32, 48, 48]             288
SynchronizedBatchNorm2d-9           [-1, 32, 48, 48]              64
SynchronizedBatchNorm2d-10           [-1, 32, 48, 48]              64
            ReLU6-11           [-1, 32, 48, 48]               0
            ReLU6-12           [-1, 32, 48, 48]               0
           Conv2d-13           [-1, 16, 48, 48]             512
           Conv2d-14           [-1, 16, 48, 48]             512
SynchronizedBatchNorm2d-15           [-1, 16, 48, 48]              32
SynchronizedBatchNorm2d-16           [-1, 16, 48, 48]              32
 InvertedResidual-17           [-1, 16, 48, 48]               0
 InvertedResidual-18           [-1, 16, 48, 48]               0
           Conv2d-19           [-1, 96, 50, 50]           1,536
           Conv2d-20           [-1, 96, 50, 50]           1,536
SynchronizedBatchNorm2d-21           [-1, 96, 50, 50]             192
SynchronizedBatchNorm2d-22           [-1, 96, 50, 50]             192
            ReLU6-23           [-1, 96, 50, 50]               0
            ReLU6-24           [-1, 96, 50, 50]               0
           Conv2d-25           [-1, 96, 24, 24]             864
           Conv2d-26           [-1, 96, 24, 24]             864
SynchronizedBatchNorm2d-27           [-1, 96, 24, 24]             192
SynchronizedBatchNorm2d-28           [-1, 96, 24, 24]             192
            ReLU6-29           [-1, 96, 24, 24]               0
            ReLU6-30           [-1, 96, 24, 24]               0
           Conv2d-31           [-1, 24, 24, 24]           2,304
           Conv2d-32           [-1, 24, 24, 24]           2,304
SynchronizedBatchNorm2d-33           [-1, 24, 24, 24]              48
SynchronizedBatchNorm2d-34           [-1, 24, 24, 24]              48
 InvertedResidual-35           [-1, 24, 24, 24]               0
 InvertedResidual-36           [-1, 24, 24, 24]               0
           Conv2d-37          [-1, 144, 26, 26]           3,456
           Conv2d-38          [-1, 144, 26, 26]           3,456
SynchronizedBatchNorm2d-39          [-1, 144, 26, 26]             288
SynchronizedBatchNorm2d-40          [-1, 144, 26, 26]             288
            ReLU6-41          [-1, 144, 26, 26]               0
            ReLU6-42          [-1, 144, 26, 26]               0
           Conv2d-43          [-1, 144, 24, 24]           1,296
           Conv2d-44          [-1, 144, 24, 24]           1,296
SynchronizedBatchNorm2d-45          [-1, 144, 24, 24]             288
SynchronizedBatchNorm2d-46          [-1, 144, 24, 24]             288
            ReLU6-47          [-1, 144, 24, 24]               0
            ReLU6-48          [-1, 144, 24, 24]               0
           Conv2d-49           [-1, 24, 24, 24]           3,456
           Conv2d-50           [-1, 24, 24, 24]           3,456
SynchronizedBatchNorm2d-51           [-1, 24, 24, 24]              48
SynchronizedBatchNorm2d-52           [-1, 24, 24, 24]              48
 InvertedResidual-53           [-1, 24, 24, 24]               0
 InvertedResidual-54           [-1, 24, 24, 24]               0
           Conv2d-55          [-1, 144, 26, 26]           3,456
           Conv2d-56          [-1, 144, 26, 26]           3,456
SynchronizedBatchNorm2d-57          [-1, 144, 26, 26]             288
SynchronizedBatchNorm2d-58          [-1, 144, 26, 26]             288
            ReLU6-59          [-1, 144, 26, 26]               0
            ReLU6-60          [-1, 144, 26, 26]               0
           Conv2d-61          [-1, 144, 12, 12]           1,296
           Conv2d-62          [-1, 144, 12, 12]           1,296
SynchronizedBatchNorm2d-63          [-1, 144, 12, 12]             288
SynchronizedBatchNorm2d-64          [-1, 144, 12, 12]             288
            ReLU6-65          [-1, 144, 12, 12]               0
            ReLU6-66          [-1, 144, 12, 12]               0
           Conv2d-67           [-1, 32, 12, 12]           4,608
           Conv2d-68           [-1, 32, 12, 12]           4,608
SynchronizedBatchNorm2d-69           [-1, 32, 12, 12]              64
SynchronizedBatchNorm2d-70           [-1, 32, 12, 12]              64
 InvertedResidual-71           [-1, 32, 12, 12]               0
 InvertedResidual-72           [-1, 32, 12, 12]               0
           Conv2d-73          [-1, 192, 14, 14]           6,144
           Conv2d-74          [-1, 192, 14, 14]           6,144
SynchronizedBatchNorm2d-75          [-1, 192, 14, 14]             384
SynchronizedBatchNorm2d-76          [-1, 192, 14, 14]             384
            ReLU6-77          [-1, 192, 14, 14]               0
            ReLU6-78          [-1, 192, 14, 14]               0
           Conv2d-79          [-1, 192, 12, 12]           1,728
           Conv2d-80          [-1, 192, 12, 12]           1,728
SynchronizedBatchNorm2d-81          [-1, 192, 12, 12]             384
SynchronizedBatchNorm2d-82          [-1, 192, 12, 12]             384
            ReLU6-83          [-1, 192, 12, 12]               0
            ReLU6-84          [-1, 192, 12, 12]               0
           Conv2d-85           [-1, 32, 12, 12]           6,144
           Conv2d-86           [-1, 32, 12, 12]           6,144
SynchronizedBatchNorm2d-87           [-1, 32, 12, 12]              64
SynchronizedBatchNorm2d-88           [-1, 32, 12, 12]              64
 InvertedResidual-89           [-1, 32, 12, 12]               0
 InvertedResidual-90           [-1, 32, 12, 12]               0
           Conv2d-91          [-1, 192, 14, 14]           6,144
           Conv2d-92          [-1, 192, 14, 14]           6,144
SynchronizedBatchNorm2d-93          [-1, 192, 14, 14]             384
SynchronizedBatchNorm2d-94          [-1, 192, 14, 14]             384
            ReLU6-95          [-1, 192, 14, 14]               0
            ReLU6-96          [-1, 192, 14, 14]               0
           Conv2d-97          [-1, 192, 12, 12]           1,728
           Conv2d-98          [-1, 192, 12, 12]           1,728
SynchronizedBatchNorm2d-99          [-1, 192, 12, 12]             384
SynchronizedBatchNorm2d-100          [-1, 192, 12, 12]             384
           ReLU6-101          [-1, 192, 12, 12]               0
           ReLU6-102          [-1, 192, 12, 12]               0
          Conv2d-103           [-1, 32, 12, 12]           6,144
          Conv2d-104           [-1, 32, 12, 12]           6,144
SynchronizedBatchNorm2d-105           [-1, 32, 12, 12]              64
SynchronizedBatchNorm2d-106           [-1, 32, 12, 12]              64
InvertedResidual-107           [-1, 32, 12, 12]               0
InvertedResidual-108           [-1, 32, 12, 12]               0
          Conv2d-109          [-1, 192, 14, 14]           6,144
          Conv2d-110          [-1, 192, 14, 14]           6,144
SynchronizedBatchNorm2d-111          [-1, 192, 14, 14]             384
SynchronizedBatchNorm2d-112          [-1, 192, 14, 14]             384
           ReLU6-113          [-1, 192, 14, 14]               0
           ReLU6-114          [-1, 192, 14, 14]               0
          Conv2d-115            [-1, 192, 6, 6]           1,728
          Conv2d-116            [-1, 192, 6, 6]           1,728
SynchronizedBatchNorm2d-117            [-1, 192, 6, 6]             384
SynchronizedBatchNorm2d-118            [-1, 192, 6, 6]             384
           ReLU6-119            [-1, 192, 6, 6]               0
           ReLU6-120            [-1, 192, 6, 6]               0
          Conv2d-121             [-1, 64, 6, 6]          12,288
          Conv2d-122             [-1, 64, 6, 6]          12,288
SynchronizedBatchNorm2d-123             [-1, 64, 6, 6]             128
SynchronizedBatchNorm2d-124             [-1, 64, 6, 6]             128
InvertedResidual-125             [-1, 64, 6, 6]               0
InvertedResidual-126             [-1, 64, 6, 6]               0
          Conv2d-127            [-1, 384, 8, 8]          24,576
          Conv2d-128            [-1, 384, 8, 8]          24,576
SynchronizedBatchNorm2d-129            [-1, 384, 8, 8]             768
SynchronizedBatchNorm2d-130            [-1, 384, 8, 8]             768
           ReLU6-131            [-1, 384, 8, 8]               0
           ReLU6-132            [-1, 384, 8, 8]               0
          Conv2d-133            [-1, 384, 6, 6]           3,456
          Conv2d-134            [-1, 384, 6, 6]           3,456
SynchronizedBatchNorm2d-135            [-1, 384, 6, 6]             768
SynchronizedBatchNorm2d-136            [-1, 384, 6, 6]             768
           ReLU6-137            [-1, 384, 6, 6]               0
           ReLU6-138            [-1, 384, 6, 6]               0
          Conv2d-139             [-1, 64, 6, 6]          24,576
          Conv2d-140             [-1, 64, 6, 6]          24,576
SynchronizedBatchNorm2d-141             [-1, 64, 6, 6]             128
SynchronizedBatchNorm2d-142             [-1, 64, 6, 6]             128
InvertedResidual-143             [-1, 64, 6, 6]               0
InvertedResidual-144             [-1, 64, 6, 6]               0
          Conv2d-145            [-1, 384, 8, 8]          24,576
          Conv2d-146            [-1, 384, 8, 8]          24,576
SynchronizedBatchNorm2d-147            [-1, 384, 8, 8]             768
SynchronizedBatchNorm2d-148            [-1, 384, 8, 8]             768
           ReLU6-149            [-1, 384, 8, 8]               0
           ReLU6-150            [-1, 384, 8, 8]               0
          Conv2d-151            [-1, 384, 6, 6]           3,456
          Conv2d-152            [-1, 384, 6, 6]           3,456
SynchronizedBatchNorm2d-153            [-1, 384, 6, 6]             768
SynchronizedBatchNorm2d-154            [-1, 384, 6, 6]             768
           ReLU6-155            [-1, 384, 6, 6]               0
           ReLU6-156            [-1, 384, 6, 6]               0
          Conv2d-157             [-1, 64, 6, 6]          24,576
          Conv2d-158             [-1, 64, 6, 6]          24,576
SynchronizedBatchNorm2d-159             [-1, 64, 6, 6]             128
SynchronizedBatchNorm2d-160             [-1, 64, 6, 6]             128
InvertedResidual-161             [-1, 64, 6, 6]               0
InvertedResidual-162             [-1, 64, 6, 6]               0
          Conv2d-163            [-1, 384, 8, 8]          24,576
          Conv2d-164            [-1, 384, 8, 8]          24,576
SynchronizedBatchNorm2d-165            [-1, 384, 8, 8]             768
SynchronizedBatchNorm2d-166            [-1, 384, 8, 8]             768
           ReLU6-167            [-1, 384, 8, 8]               0
           ReLU6-168            [-1, 384, 8, 8]               0
          Conv2d-169            [-1, 384, 6, 6]           3,456
          Conv2d-170            [-1, 384, 6, 6]           3,456
SynchronizedBatchNorm2d-171            [-1, 384, 6, 6]             768
SynchronizedBatchNorm2d-172            [-1, 384, 6, 6]             768
           ReLU6-173            [-1, 384, 6, 6]               0
           ReLU6-174            [-1, 384, 6, 6]               0
          Conv2d-175             [-1, 64, 6, 6]          24,576
          Conv2d-176             [-1, 64, 6, 6]          24,576
SynchronizedBatchNorm2d-177             [-1, 64, 6, 6]             128
SynchronizedBatchNorm2d-178             [-1, 64, 6, 6]             128
InvertedResidual-179             [-1, 64, 6, 6]               0
InvertedResidual-180             [-1, 64, 6, 6]               0
          Conv2d-181            [-1, 384, 8, 8]          24,576
          Conv2d-182            [-1, 384, 8, 8]          24,576
SynchronizedBatchNorm2d-183            [-1, 384, 8, 8]             768
SynchronizedBatchNorm2d-184            [-1, 384, 8, 8]             768
           ReLU6-185            [-1, 384, 8, 8]               0
           ReLU6-186            [-1, 384, 8, 8]               0
          Conv2d-187            [-1, 384, 6, 6]           3,456
          Conv2d-188            [-1, 384, 6, 6]           3,456
SynchronizedBatchNorm2d-189            [-1, 384, 6, 6]             768
SynchronizedBatchNorm2d-190            [-1, 384, 6, 6]             768
           ReLU6-191            [-1, 384, 6, 6]               0
           ReLU6-192            [-1, 384, 6, 6]               0
          Conv2d-193             [-1, 96, 6, 6]          36,864
          Conv2d-194             [-1, 96, 6, 6]          36,864
SynchronizedBatchNorm2d-195             [-1, 96, 6, 6]             192
SynchronizedBatchNorm2d-196             [-1, 96, 6, 6]             192
InvertedResidual-197             [-1, 96, 6, 6]               0
InvertedResidual-198             [-1, 96, 6, 6]               0
          Conv2d-199            [-1, 576, 8, 8]          55,296
          Conv2d-200            [-1, 576, 8, 8]          55,296
SynchronizedBatchNorm2d-201            [-1, 576, 8, 8]           1,152
SynchronizedBatchNorm2d-202            [-1, 576, 8, 8]           1,152
           ReLU6-203            [-1, 576, 8, 8]               0
           ReLU6-204            [-1, 576, 8, 8]               0
          Conv2d-205            [-1, 576, 6, 6]           5,184
          Conv2d-206            [-1, 576, 6, 6]           5,184
SynchronizedBatchNorm2d-207            [-1, 576, 6, 6]           1,152
SynchronizedBatchNorm2d-208            [-1, 576, 6, 6]           1,152
           ReLU6-209            [-1, 576, 6, 6]               0
           ReLU6-210            [-1, 576, 6, 6]               0
          Conv2d-211             [-1, 96, 6, 6]          55,296
          Conv2d-212             [-1, 96, 6, 6]          55,296
SynchronizedBatchNorm2d-213             [-1, 96, 6, 6]             192
SynchronizedBatchNorm2d-214             [-1, 96, 6, 6]             192
InvertedResidual-215             [-1, 96, 6, 6]               0
InvertedResidual-216             [-1, 96, 6, 6]               0
          Conv2d-217            [-1, 576, 8, 8]          55,296
          Conv2d-218            [-1, 576, 8, 8]          55,296
SynchronizedBatchNorm2d-219            [-1, 576, 8, 8]           1,152
SynchronizedBatchNorm2d-220            [-1, 576, 8, 8]           1,152
           ReLU6-221            [-1, 576, 8, 8]               0
           ReLU6-222            [-1, 576, 8, 8]               0
          Conv2d-223            [-1, 576, 6, 6]           5,184
          Conv2d-224            [-1, 576, 6, 6]           5,184
SynchronizedBatchNorm2d-225            [-1, 576, 6, 6]           1,152
SynchronizedBatchNorm2d-226            [-1, 576, 6, 6]           1,152
           ReLU6-227            [-1, 576, 6, 6]               0
           ReLU6-228            [-1, 576, 6, 6]               0
          Conv2d-229             [-1, 96, 6, 6]          55,296
          Conv2d-230             [-1, 96, 6, 6]          55,296
SynchronizedBatchNorm2d-231             [-1, 96, 6, 6]             192
SynchronizedBatchNorm2d-232             [-1, 96, 6, 6]             192
InvertedResidual-233             [-1, 96, 6, 6]               0
InvertedResidual-234             [-1, 96, 6, 6]               0
          Conv2d-235            [-1, 576, 8, 8]          55,296
          Conv2d-236            [-1, 576, 8, 8]          55,296
SynchronizedBatchNorm2d-237            [-1, 576, 8, 8]           1,152
SynchronizedBatchNorm2d-238            [-1, 576, 8, 8]           1,152
           ReLU6-239            [-1, 576, 8, 8]               0
           ReLU6-240            [-1, 576, 8, 8]               0
          Conv2d-241            [-1, 576, 6, 6]           5,184
          Conv2d-242            [-1, 576, 6, 6]           5,184
SynchronizedBatchNorm2d-243            [-1, 576, 6, 6]           1,152
SynchronizedBatchNorm2d-244            [-1, 576, 6, 6]           1,152
           ReLU6-245            [-1, 576, 6, 6]               0
           ReLU6-246            [-1, 576, 6, 6]               0
          Conv2d-247            [-1, 160, 6, 6]          92,160
          Conv2d-248            [-1, 160, 6, 6]          92,160
SynchronizedBatchNorm2d-249            [-1, 160, 6, 6]             320
SynchronizedBatchNorm2d-250            [-1, 160, 6, 6]             320
InvertedResidual-251            [-1, 160, 6, 6]               0
InvertedResidual-252            [-1, 160, 6, 6]               0
          Conv2d-253            [-1, 960, 8, 8]         153,600
          Conv2d-254            [-1, 960, 8, 8]         153,600
SynchronizedBatchNorm2d-255            [-1, 960, 8, 8]           1,920
SynchronizedBatchNorm2d-256            [-1, 960, 8, 8]           1,920
           ReLU6-257            [-1, 960, 8, 8]               0
           ReLU6-258            [-1, 960, 8, 8]               0
          Conv2d-259            [-1, 960, 6, 6]           8,640
          Conv2d-260            [-1, 960, 6, 6]           8,640
SynchronizedBatchNorm2d-261            [-1, 960, 6, 6]           1,920
SynchronizedBatchNorm2d-262            [-1, 960, 6, 6]           1,920
           ReLU6-263            [-1, 960, 6, 6]               0
           ReLU6-264            [-1, 960, 6, 6]               0
          Conv2d-265            [-1, 160, 6, 6]         153,600
          Conv2d-266            [-1, 160, 6, 6]         153,600
SynchronizedBatchNorm2d-267            [-1, 160, 6, 6]             320
SynchronizedBatchNorm2d-268            [-1, 160, 6, 6]             320
InvertedResidual-269            [-1, 160, 6, 6]               0
InvertedResidual-270            [-1, 160, 6, 6]               0
          Conv2d-271            [-1, 960, 8, 8]         153,600
          Conv2d-272            [-1, 960, 8, 8]         153,600
SynchronizedBatchNorm2d-273            [-1, 960, 8, 8]           1,920
SynchronizedBatchNorm2d-274            [-1, 960, 8, 8]           1,920
           ReLU6-275            [-1, 960, 8, 8]               0
           ReLU6-276            [-1, 960, 8, 8]               0
          Conv2d-277            [-1, 960, 6, 6]           8,640
          Conv2d-278            [-1, 960, 6, 6]           8,640
SynchronizedBatchNorm2d-279            [-1, 960, 6, 6]           1,920
SynchronizedBatchNorm2d-280            [-1, 960, 6, 6]           1,920
           ReLU6-281            [-1, 960, 6, 6]               0
           ReLU6-282            [-1, 960, 6, 6]               0
          Conv2d-283            [-1, 160, 6, 6]         153,600
          Conv2d-284            [-1, 160, 6, 6]         153,600
SynchronizedBatchNorm2d-285            [-1, 160, 6, 6]             320
SynchronizedBatchNorm2d-286            [-1, 160, 6, 6]             320
InvertedResidual-287            [-1, 160, 6, 6]               0
InvertedResidual-288            [-1, 160, 6, 6]               0
          Conv2d-289          [-1, 960, 10, 10]         153,600
          Conv2d-290          [-1, 960, 10, 10]         153,600
SynchronizedBatchNorm2d-291          [-1, 960, 10, 10]           1,920
SynchronizedBatchNorm2d-292          [-1, 960, 10, 10]           1,920
           ReLU6-293          [-1, 960, 10, 10]               0
           ReLU6-294          [-1, 960, 10, 10]               0
          Conv2d-295            [-1, 960, 6, 6]           8,640
          Conv2d-296            [-1, 960, 6, 6]           8,640
SynchronizedBatchNorm2d-297            [-1, 960, 6, 6]           1,920
SynchronizedBatchNorm2d-298            [-1, 960, 6, 6]           1,920
           ReLU6-299            [-1, 960, 6, 6]               0
           ReLU6-300            [-1, 960, 6, 6]               0
          Conv2d-301            [-1, 320, 6, 6]         307,200
          Conv2d-302            [-1, 320, 6, 6]         307,200
SynchronizedBatchNorm2d-303            [-1, 320, 6, 6]             640
SynchronizedBatchNorm2d-304            [-1, 320, 6, 6]             640
InvertedResidual-305            [-1, 320, 6, 6]               0
InvertedResidual-306            [-1, 320, 6, 6]               0
     MobileNetV2-307  [[-1, 320, 6, 6], [-1, 24, 24, 24]]               0
          Conv2d-308            [-1, 256, 6, 6]          81,920
SynchronizedBatchNorm2d-309            [-1, 256, 6, 6]             512
            ReLU-310            [-1, 256, 6, 6]               0
     _ASPPModule-311            [-1, 256, 6, 6]               0
          Conv2d-312            [-1, 256, 6, 6]         737,280
SynchronizedBatchNorm2d-313            [-1, 256, 6, 6]             512
            ReLU-314            [-1, 256, 6, 6]               0
     _ASPPModule-315            [-1, 256, 6, 6]               0
          Conv2d-316            [-1, 256, 6, 6]         737,280
SynchronizedBatchNorm2d-317            [-1, 256, 6, 6]             512
            ReLU-318            [-1, 256, 6, 6]               0
     _ASPPModule-319            [-1, 256, 6, 6]               0
          Conv2d-320            [-1, 256, 6, 6]         737,280
SynchronizedBatchNorm2d-321            [-1, 256, 6, 6]             512
            ReLU-322            [-1, 256, 6, 6]               0
     _ASPPModule-323            [-1, 256, 6, 6]               0
AdaptiveAvgPool2d-324            [-1, 320, 1, 1]               0
          Conv2d-325            [-1, 256, 1, 1]          81,920
SynchronizedBatchNorm2d-326            [-1, 256, 1, 1]             512
            ReLU-327            [-1, 256, 1, 1]               0
          Conv2d-328            [-1, 256, 6, 6]         327,680
SynchronizedBatchNorm2d-329            [-1, 256, 6, 6]             512
            ReLU-330            [-1, 256, 6, 6]               0
         Dropout-331            [-1, 256, 6, 6]               0
            ASPP-332            [-1, 256, 6, 6]               0
          Conv2d-333           [-1, 48, 24, 24]           1,152
SynchronizedBatchNorm2d-334           [-1, 48, 24, 24]              96
            ReLU-335           [-1, 48, 24, 24]               0
          Conv2d-336          [-1, 256, 24, 24]         700,416
SynchronizedBatchNorm2d-337          [-1, 256, 24, 24]             512
            ReLU-338          [-1, 256, 24, 24]               0
         Dropout-339          [-1, 256, 24, 24]               0
          Conv2d-340          [-1, 256, 24, 24]         589,824
SynchronizedBatchNorm2d-341          [-1, 256, 24, 24]             512
            ReLU-342          [-1, 256, 24, 24]               0
         Dropout-343          [-1, 256, 24, 24]               0
          Conv2d-344            [-1, 1, 24, 24]             257
         Decoder-345            [-1, 1, 24, 24]               0
================================================================
Total params: 7,621,473
Trainable params: 7,621,473
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.04
Forward/backward pass size (MB): 1126.42
Params size (MB): 29.07
Estimated Total Size (MB): 1155.53
----------------------------------------------------------------
horizontal_splits_number 66
width_after_pad 3216
left_pad,right_pad 21 21
vertical_splits_number 32
height_after_pad 1584
top_pad,bottom_pad 23 24
181029
181029
(96, 96)
read images in 195.48854088783264 sec
(181029, 96, 96)
(181029, 96, 96)
read images in 13.18459415435791 sec
(181029, 96, 96, 1)
(181029, 96, 96, 1)
64317
64317
(96, 96)
read images in 22.29970359802246 sec
(64317, 96, 96)
(64317, 96, 96)
read images in 3.7916336059570312 sec
(64317, 96, 96, 1)
(64317, 96, 96, 1)
X_train (181029, 96, 96, 1)
X_val (64317, 96, 96, 1)
Y_train (181029, 96, 96, 1)
Y_val (64317, 96, 96, 1)
X_train (181029, 1, 96, 96)
X_val (64317, 1, 96, 96)
Y_train (181029, 1, 96, 96)
Y_val (64317, 1, 96, 96)
optimizer = torch.optim.SGD(model.parameters(), lr=1e-6, momentum=0.9, weight_decay=0.0002)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
Validation loss decreased (inf --> 0.281769).  Saving model ...
Epoch: 1. Train Loss: 0.22265852987766266. Val Loss: 0.2817685902118683. Train IoU: 0.2939518392086029. Val IoU: 0.1877145916223526. Time: 1585.635088443756. LR: 0.01
Validation loss decreased (0.281769 --> 0.231759).  Saving model ...
Epoch: 2. Train Loss: 0.17223508656024933. Val Loss: 0.23175926506519318. Train IoU: 0.4295143783092499. Val IoU: 0.30506256222724915. Time: 1474.518870830536. LR: 0.01
EarlyStopping counter: 1 out of 20
Epoch: 3. Train Loss: 0.15664014220237732. Val Loss: 0.3324238359928131. Train IoU: 0.474579781293869. Val IoU: 0.18652968108654022. Time: 1462.409199476242. LR: 0.01
Validation loss decreased (0.231759 --> 0.228389).  Saving model ...
Epoch: 4. Train Loss: 0.14437973499298096. Val Loss: 0.22838912904262543. Train IoU: 0.5101413130760193. Val IoU: 0.3611092269420624. Time: 1443.4804112911224. LR: 0.01
EarlyStopping counter: 1 out of 20
Epoch: 5. Train Loss: 0.13433171808719635. Val Loss: 0.3353058993816376. Train IoU: 0.5405348539352417. Val IoU: 0.2826778292655945. Time: 1439.715765953064. LR: 0.01
EarlyStopping counter: 2 out of 20
Epoch: 6. Train Loss: 0.12518776953220367. Val Loss: 0.260931134223938. Train IoU: 0.5702357888221741. Val IoU: 0.2912927567958832. Time: 1427.141462802887. LR: 0.01
EarlyStopping counter: 3 out of 20
Epoch: 7. Train Loss: 0.11788540333509445. Val Loss: 0.25007760524749756. Train IoU: 0.5935202836990356. Val IoU: 0.37781020998954773. Time: 1489.2868392467499. LR: 0.01
EarlyStopping counter: 4 out of 20
Epoch: 8. Train Loss: 0.11155704408884048. Val Loss: 0.27970820665359497. Train IoU: 0.6140332818031311. Val IoU: 0.327186644077301. Time: 1460.2738816738129. LR: 0.01
EarlyStopping counter: 5 out of 20
Epoch: 9. Train Loss: 0.10611562430858612. Val Loss: 0.2573981285095215. Train IoU: 0.631872296333313. Val IoU: 0.388359010219574. Time: 1458.1892075538635. LR: 0.01
Epoch     9: reducing learning rate of group 0 to 1.0000e-03.
EarlyStopping counter: 6 out of 20
Epoch: 10. Train Loss: 0.10198534280061722. Val Loss: 0.25920185446739197. Train IoU: 0.6451823115348816. Val IoU: 0.39748045802116394. Time: 1444.7199442386627. LR: 0.001
EarlyStopping counter: 7 out of 20
Epoch: 11. Train Loss: 0.08466178178787231. Val Loss: 0.2927929162979126. Train IoU: 0.6988992691040039. Val IoU: 0.4027025103569031. Time: 1450.0758850574493. LR: 0.001
EarlyStopping counter: 8 out of 20
Epoch: 12. Train Loss: 0.07831647992134094. Val Loss: 0.3030932545661926. Train IoU: 0.7183129191398621. Val IoU: 0.4075893759727478. Time: 1420.0705997943878. LR: 0.001
EarlyStopping counter: 9 out of 20
Epoch: 13. Train Loss: 0.07517530024051666. Val Loss: 0.32139456272125244. Train IoU: 0.7282865047454834. Val IoU: 0.39877936244010925. Time: 1494.5414998531342. LR: 0.001
EarlyStopping counter: 10 out of 20
Epoch: 14. Train Loss: 0.07268307358026505. Val Loss: 0.32867899537086487. Train IoU: 0.7362489104270935. Val IoU: 0.402129203081131. Time: 1456.1876327991486. LR: 0.001
EarlyStopping counter: 11 out of 20
Epoch: 15. Train Loss: 0.070973239839077. Val Loss: 0.33631089329719543. Train IoU: 0.7414500713348389. Val IoU: 0.40440016984939575. Time: 1450.9850306510925. LR: 0.001
Epoch    15: reducing learning rate of group 0 to 1.0000e-04.
EarlyStopping counter: 12 out of 20
Epoch: 16. Train Loss: 0.06927770376205444. Val Loss: 0.34530144929885864. Train IoU: 0.7471370100975037. Val IoU: 0.4003545045852661. Time: 1440.8664882183075. LR: 0.0001
EarlyStopping counter: 13 out of 20
Epoch: 17. Train Loss: 0.06683468073606491. Val Loss: 0.36048710346221924. Train IoU: 0.7550024390220642. Val IoU: 0.4007325768470764. Time: 1685.934585571289. LR: 0.0001
EarlyStopping counter: 14 out of 20
Epoch: 18. Train Loss: 0.06634385883808136. Val Loss: 0.363852858543396. Train IoU: 0.7565262913703918. Val IoU: 0.39948520064353943. Time: 1891.116842508316. LR: 0.0001
EarlyStopping counter: 15 out of 20
Epoch: 19. Train Loss: 0.06598268449306488. Val Loss: 0.357296347618103. Train IoU: 0.7576383352279663. Val IoU: 0.4037180244922638. Time: 1442.043271780014. LR: 0.0001
EarlyStopping counter: 16 out of 20
Epoch: 20. Train Loss: 0.06574142724275589. Val Loss: 0.36304089426994324. Train IoU: 0.758591890335083. Val IoU: 0.40428218245506287. Time: 1420.6568667888641. LR: 0.0001
EarlyStopping counter: 17 out of 20
Epoch: 21. Train Loss: 0.06553913652896881. Val Loss: 0.36769092082977295. Train IoU: 0.7591363191604614. Val IoU: 0.4008248448371887. Time: 1438.9990162849426. LR: 0.0001
Epoch    21: reducing learning rate of group 0 to 1.0000e-05.
EarlyStopping counter: 18 out of 20
Epoch: 22. Train Loss: 0.06537976861000061. Val Loss: 0.367109477519989. Train IoU: 0.7596641182899475. Val IoU: 0.4010578393936157. Time: 1453.3729441165924. LR: 1e-05
EarlyStopping counter: 19 out of 20
Epoch: 23. Train Loss: 0.06510692089796066. Val Loss: 0.36244121193885803. Train IoU: 0.7606468200683594. Val IoU: 0.4029514491558075. Time: 1426.1785397529602. LR: 1e-05
EarlyStopping counter: 20 out of 20
Early stopping
total cost 9.96461668756273 hours
/home/anyu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:763: UserWarning: Attempting to work in a virtualenv. If you encounter problems, please install IPython inside the virtualenv.
  warn("Attempting to work in a virtualenv. If you encounter problems, please "
